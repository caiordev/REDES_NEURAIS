{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F√≥rmulas Importantes no Backpropagation - Parte 2\n",
    "\n",
    "## Introdu√ß√£o\n",
    "\n",
    "Neste notebook, exploraremos as f√≥rmulas matem√°ticas para o c√°lculo do erro nas camadas ocultas e a atualiza√ß√£o dos pesos durante o algoritmo de Backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o das bibliotecas necess√°rias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display, Math\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erro nas Camadas Ocultas\n",
    "\n",
    "Enquanto o erro na camada de sa√≠da √© calculado diretamente comparando a sa√≠da da rede com o valor desejado, o erro nas camadas ocultas √© calculado propagando o erro da camada seguinte para tr√°s.\n",
    "\n",
    "### Deriva√ß√£o da F√≥rmula\n",
    "\n",
    "Para um neur√¥nio $j$ na camada $l$, o erro $\\delta_j^l$ √© definido como a derivada parcial do erro total em rela√ß√£o √† soma ponderada $z_j^l$:\n",
    "\n",
    "$\\delta_j^l = \\frac{\\partial E}{\\partial z_j^l}$\n",
    "\n",
    "Usando a regra da cadeia, podemos expressar este erro em termos do erro na camada seguinte $l+1$:\n",
    "\n",
    "$\\delta_j^l = \\left( \\sum_{k} w_{kj}^{l+1} \\delta_k^{l+1} \\right) \\cdot f'(z_j^l)$\n",
    "\n",
    "Onde:\n",
    "- $w_{kj}^{l+1}$ √© o peso da conex√£o entre o neur√¥nio $j$ na camada $l$ e o neur√¥nio $k$ na camada $l+1$\n",
    "- $\\delta_k^{l+1}$ √© o erro do neur√¥nio $k$ na camada $l+1$\n",
    "- $f'(z_j^l)$ √© a derivada da fun√ß√£o de ativa√ß√£o avaliada em $z_j^l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= INICIANDO C√ÅLCULO DO ERRO NA CAMADA OCULTA =================\n",
      "\n",
      "1Ô∏è‚É£ Dados de entrada:\n",
      "üî∏ Erro na camada seguinte (delta_next):\n",
      "[[0.1]\n",
      " [0.2]\n",
      " [0.3]]\n",
      "üî∏ Pesos da camada atual para a pr√≥xima (weights_next):\n",
      "[[0.1 0.2]\n",
      " [0.3 0.4]\n",
      " [0.5 0.6]]\n",
      "üî∏ Soma ponderada das entradas da camada atual (z_current):\n",
      "[[0.5]\n",
      " [0.7]]\n",
      "üî∏ Fun√ß√£o de ativa√ß√£o utilizada: sigmoid\n",
      "\n",
      "2Ô∏è‚É£ Contribui√ß√£o do erro da camada seguinte (peso.T x delta_next):\n",
      "[[0.22]\n",
      " [0.28]]\n",
      "\n",
      "3Ô∏è‚É£ Derivada da fun√ß√£o de ativa√ß√£o aplicada √† camada atual:\n",
      "[[0.23500371]\n",
      " [0.22171287]]\n",
      "\n",
      "4Ô∏è‚É£ Resultado final - Erro na camada atual (delta):\n",
      "[[0.05170082]\n",
      " [0.0620796 ]]\n",
      "\n",
      "================= FIM DO C√ÅLCULO =================\n",
      "\n",
      "Delta (erro) na camada atual calculado com sucesso:\n",
      "[[0.05170082]\n",
      " [0.0620796 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fun√ß√µes auxiliares\n",
    "def sigmoid(z):\n",
    "    \"\"\"Fun√ß√£o de ativa√ß√£o sigmoid\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivada da fun√ß√£o sigmoid\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# C√°lculo do erro na camada oculta\n",
    "def hidden_layer_error(delta_next, weights_next, z_current, activation_function='sigmoid'):\n",
    "    \"\"\"Calcula o erro para uma camada oculta\"\"\"\n",
    "    \n",
    "    print(\"\\n================= INICIANDO C√ÅLCULO DO ERRO NA CAMADA OCULTA =================\")\n",
    "    \n",
    "    print(\"\\n1Ô∏è‚É£ Dados de entrada:\")\n",
    "    print(f\"üî∏ Erro na camada seguinte (delta_next):\\n{delta_next}\")\n",
    "    print(f\"üî∏ Pesos da camada atual para a pr√≥xima (weights_next):\\n{weights_next}\")\n",
    "    print(f\"üî∏ Soma ponderada das entradas da camada atual (z_current):\\n{z_current}\")\n",
    "    print(f\"üî∏ Fun√ß√£o de ativa√ß√£o utilizada: {activation_function}\")\n",
    "    \n",
    "    # 1. Calcula contribui√ß√£o do erro da pr√≥xima camada\n",
    "    error_contribution = np.dot(weights_next.T, delta_next)\n",
    "    print(\"\\n2Ô∏è‚É£ Contribui√ß√£o do erro da camada seguinte (peso.T x delta_next):\")\n",
    "    print(error_contribution)\n",
    "    \n",
    "    # 2. Calcula a derivada da fun√ß√£o de ativa√ß√£o\n",
    "    if activation_function == 'sigmoid':\n",
    "        activation_derivative = sigmoid_derivative(z_current)\n",
    "    elif activation_function == 'tanh':\n",
    "        activation_derivative = 1 - np.tanh(z_current) ** 2\n",
    "    elif activation_function == 'relu':\n",
    "        activation_derivative = np.where(z_current > 0, 1, 0)\n",
    "    else:\n",
    "        raise ValueError(f\"Fun√ß√£o de ativa√ß√£o '{activation_function}' n√£o suportada\")\n",
    "    \n",
    "    print(\"\\n3Ô∏è‚É£ Derivada da fun√ß√£o de ativa√ß√£o aplicada √† camada atual:\")\n",
    "    print(activation_derivative)\n",
    "    \n",
    "    # 3. Calcula o erro da camada atual\n",
    "    delta = error_contribution * activation_derivative\n",
    "    \n",
    "    print(\"\\n4Ô∏è‚É£ Resultado final - Erro na camada atual (delta):\")\n",
    "    print(delta)\n",
    "    \n",
    "    print(\"\\n================= FIM DO C√ÅLCULO =================\\n\")\n",
    "    return delta\n",
    "\n",
    "# ================= EXEMPLO DE USO =================\n",
    "# Rede com 3 neur√¥nios na camada l+1 e 2 neur√¥nios na camada l\n",
    "delta_next = np.array([[0.1], [0.2], [0.3]])  # Erro na camada seguinte (l+1)\n",
    "weights_next = np.array([[0.1, 0.2],   # Pesos de cada neur√¥nio da camada l+1 para camada l\n",
    "                          [0.3, 0.4],\n",
    "                          [0.5, 0.6]])\n",
    "z_current = np.array([[0.5], [0.7]])  # Soma ponderada na camada atual (l)\n",
    "\n",
    "# C√°lculo do erro na camada atual (l)\n",
    "delta_current = hidden_layer_error(delta_next, weights_next, z_current, activation_function='sigmoid')\n",
    "\n",
    "print(\"Delta (erro) na camada atual calculado com sucesso:\")\n",
    "print(delta_current)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atualiza√ß√£o dos Pesos\n",
    "\n",
    "Ap√≥s calcular os erros para cada neur√¥nio na rede, o pr√≥ximo passo √© atualizar os pesos para minimizar o erro. Isso √© feito usando o algoritmo do gradiente descendente ou suas variantes.\n",
    "\n",
    "### Deriva√ß√£o da F√≥rmula\n",
    "\n",
    "Para atualizar um peso $w_{ji}^l$ que conecta o neur√¥nio $i$ na camada $l-1$ ao neur√¥nio $j$ na camada $l$, a f√≥rmula √©:\n",
    "\n",
    "$w_{ji}^l = w_{ji}^l - \\eta \\frac{\\partial E}{\\partial w_{ji}^l} = w_{ji}^l - \\eta \\delta_j^l a_i^{l-1}$\n",
    "\n",
    "Onde:\n",
    "- $\\eta$ √© a taxa de aprendizado\n",
    "- $\\delta_j^l$ √© o erro do neur√¥nio $j$ na camada $l$\n",
    "- $a_i^{l-1}$ √© a ativa√ß√£o do neur√¥nio $i$ na camada $l-1$\n",
    "\n",
    "De forma similar, a atualiza√ß√£o dos vieses (bias) √© dada por:\n",
    "\n",
    "$b_j^l = b_j^l - \\eta \\delta_j^l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= RESULTADOS DA ATUALIZA√á√ÉO =================\n",
      "\n",
      "üì¶ Camada 1 - PESOS\n",
      "üî∏ Pesos originais:\n",
      "[[0.1 0.2]\n",
      " [0.3 0.4]]\n",
      "üî∏ Pesos atualizados:\n",
      "[[0.095 0.194]\n",
      " [0.29  0.388]]\n",
      "üõ†Ô∏è Interpreta√ß√£o: Cada valor foi ajustado na dire√ß√£o de reduzir o erro da rede. A atualiza√ß√£o ocorre proporcional ao gradiente (delta) e √† ativa√ß√£o da camada anterior.\n",
      "\n",
      "üì¶ Camada 2 - PESOS\n",
      "üî∏ Pesos originais:\n",
      "[[0.5 0.6]]\n",
      "üî∏ Pesos atualizados:\n",
      "[[0.479 0.576]]\n",
      "üõ†Ô∏è Interpreta√ß√£o: Cada valor foi ajustado na dire√ß√£o de reduzir o erro da rede. A atualiza√ß√£o ocorre proporcional ao gradiente (delta) e √† ativa√ß√£o da camada anterior.\n",
      "\n",
      "‚öôÔ∏è Camada 1 - VIESES\n",
      "üî∏ Vieses originais:\n",
      "[[0.1]\n",
      " [0.2]]\n",
      "üî∏ Vieses atualizados:\n",
      "[[0.09]\n",
      " [0.18]]\n",
      "üõ†Ô∏è Interpreta√ß√£o: Os vieses foram ajustados diretamente na propor√ß√£o do delta da camada, independentemente da ativa√ß√£o, pois o vi√©s n√£o depende da entrada.\n",
      "\n",
      "‚öôÔ∏è Camada 2 - VIESES\n",
      "üî∏ Vieses originais:\n",
      "[[0.3]]\n",
      "üî∏ Vieses atualizados:\n",
      "[[0.27]]\n",
      "üõ†Ô∏è Interpreta√ß√£o: Os vieses foram ajustados diretamente na propor√ß√£o do delta da camada, independentemente da ativa√ß√£o, pois o vi√©s n√£o depende da entrada.\n",
      "\n",
      "================= FIM =================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Implementa√ß√£o da atualiza√ß√£o de pesos e vieses\n",
    "# =========================\n",
    "\n",
    "def update_weights_and_biases(weights, biases, deltas, activations, learning_rate):\n",
    "    \"\"\"Atualiza os pesos e vieses usando o gradiente descendente\"\"\"\n",
    "    for l in range(len(weights)):\n",
    "        weights[l] = weights[l] - learning_rate * np.dot(deltas[l], activations[l].transpose())\n",
    "        biases[l] = biases[l] - learning_rate * deltas[l]\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Exemplo de uso\n",
    "# =========================\n",
    "\n",
    "# Definir uma rede neural simples com 2 camadas\n",
    "weights = [np.array([[0.1, 0.2], [0.3, 0.4]]), np.array([[0.5, 0.6]])]\n",
    "biases = [np.array([[0.1], [0.2]]), np.array([[0.3]])]\n",
    "\n",
    "# Definir ativa√ß√µes e erros (deltas)\n",
    "activations = [np.array([[0.5], [0.6]]), np.array([[0.7], [0.8]]), np.array([[0.9]])]\n",
    "deltas = [np.array([[0.1], [0.2]]), np.array([[0.3]])]\n",
    "\n",
    "# Definir taxa de aprendizado\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Salvar os pesos e vieses originais para compara√ß√£o\n",
    "original_weights = [w.copy() for w in weights]\n",
    "original_biases = [b.copy() for b in biases]\n",
    "\n",
    "# Atualizar pesos e vieses\n",
    "new_weights, new_biases = update_weights_and_biases(weights, biases, deltas, activations, learning_rate)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Impress√£o detalhada dos resultados\n",
    "# =========================\n",
    "\n",
    "print(\"\\n================= RESULTADOS DA ATUALIZA√á√ÉO =================\\n\")\n",
    "\n",
    "# Pesos\n",
    "for i in range(len(original_weights)):\n",
    "    print(f\"üì¶ Camada {i+1} - PESOS\")\n",
    "    print(\"üî∏ Pesos originais:\")\n",
    "    print(original_weights[i])\n",
    "    print(\"üî∏ Pesos atualizados:\")\n",
    "    print(new_weights[i])\n",
    "    print(\"üõ†Ô∏è Interpreta√ß√£o: Cada valor foi ajustado na dire√ß√£o de reduzir o erro da rede. \"\n",
    "          \"A atualiza√ß√£o ocorre proporcional ao gradiente (delta) e √† ativa√ß√£o da camada anterior.\\n\")\n",
    "\n",
    "# Vieses\n",
    "for i in range(len(original_biases)):\n",
    "    print(f\"‚öôÔ∏è Camada {i+1} - VIESES\")\n",
    "    print(\"üî∏ Vieses originais:\")\n",
    "    print(original_biases[i])\n",
    "    print(\"üî∏ Vieses atualizados:\")\n",
    "    print(new_biases[i])\n",
    "    print(\"üõ†Ô∏è Interpreta√ß√£o: Os vieses foram ajustados diretamente na propor√ß√£o do delta da camada, \"\n",
    "          \"independentemente da ativa√ß√£o, pois o vi√©s n√£o depende da entrada.\\n\")\n",
    "\n",
    "print(\"================= FIM =================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variantes do Gradiente Descendente\n",
    "\n",
    "Existem v√°rias variantes do algoritmo do gradiente descendente que podem melhorar a converg√™ncia e o desempenho:\n",
    "\n",
    "### Gradiente Descendente com Momentum\n",
    "\n",
    "Adiciona um termo de momentum que ajuda a acelerar a converg√™ncia e evitar m√≠nimos locais:\n",
    "\n",
    "$v = \\gamma v - \\eta \\nabla E$\n",
    "$w = w + v$\n",
    "\n",
    "Onde $v$ √© o vetor de velocidade e $\\gamma$ √© o coeficiente de momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= RESULTADOS DA ATUALIZA√á√ÉO COM MOMENTUM =================\n",
      "\n",
      "üì¶ Camada 1 - PESOS\n",
      "üî∏ Pesos atualizados com momentum:\n",
      "[[0.085 0.182]\n",
      " [0.27  0.364]]\n",
      "üõ†Ô∏è Interpreta√ß√£o:\n",
      "Nesta camada, os pesos foram atualizados considerando n√£o apenas o gradiente atual (que aponta na dire√ß√£o do erro atual), mas tamb√©m a velocidade acumulada de atualiza√ß√µes anteriores. O momentum (coeficiente = 0.9) faz com que a rede tenha 'in√©rcia' nas atualiza√ß√µes, ajudando a acelerar nas dire√ß√µes corretas e a reduzir oscila√ß√µes, especialmente em vales estreitos do espa√ßo de erro.\n",
      "\n",
      "üì¶ Camada 2 - PESOS\n",
      "üî∏ Pesos atualizados com momentum:\n",
      "[[0.437 0.528]]\n",
      "üõ†Ô∏è Interpreta√ß√£o:\n",
      "Nesta camada, os pesos foram atualizados considerando n√£o apenas o gradiente atual (que aponta na dire√ß√£o do erro atual), mas tamb√©m a velocidade acumulada de atualiza√ß√µes anteriores. O momentum (coeficiente = 0.9) faz com que a rede tenha 'in√©rcia' nas atualiza√ß√µes, ajudando a acelerar nas dire√ß√µes corretas e a reduzir oscila√ß√µes, especialmente em vales estreitos do espa√ßo de erro.\n",
      "\n",
      "‚öôÔ∏è Camada 1 - VIESES\n",
      "üî∏ Vieses atualizados com momentum:\n",
      "[[0.07]\n",
      " [0.14]]\n",
      "üõ†Ô∏è Interpreta√ß√£o:\n",
      "Os vieses foram atualizados com a mesma l√≥gica dos pesos: al√©m do gradiente atual, a velocidade anterior influencia a dire√ß√£o e o tamanho do passo. Isso permite um avan√ßo mais suave e r√°pido na superf√≠cie de erro, evitando quedas abruptas e ajudando na estabilidade da aprendizagem.\n",
      "\n",
      "‚öôÔ∏è Camada 2 - VIESES\n",
      "üî∏ Vieses atualizados com momentum:\n",
      "[[0.21]]\n",
      "üõ†Ô∏è Interpreta√ß√£o:\n",
      "Os vieses foram atualizados com a mesma l√≥gica dos pesos: al√©m do gradiente atual, a velocidade anterior influencia a dire√ß√£o e o tamanho do passo. Isso permite um avan√ßo mais suave e r√°pido na superf√≠cie de erro, evitando quedas abruptas e ajudando na estabilidade da aprendizagem.\n",
      "\n",
      "üöÄ Camada 1 - VELOCIDADES DOS PESOS\n",
      "[[-0.005 -0.006]\n",
      " [-0.01  -0.012]]\n",
      "üåÄ Estas velocidades representam a combina√ß√£o entre a velocidade anterior e o gradiente atual, ponderadas pelo momentum. Quanto maior o momentum, mais a velocidade anterior influencia.\n",
      "\n",
      "üöÄ Camada 2 - VELOCIDADES DOS PESOS\n",
      "[[-0.021 -0.024]]\n",
      "üåÄ Estas velocidades representam a combina√ß√£o entre a velocidade anterior e o gradiente atual, ponderadas pelo momentum. Quanto maior o momentum, mais a velocidade anterior influencia.\n",
      "\n",
      "üöÄ Camada 1 - VELOCIDADES DOS VIESES\n",
      "[[-0.01]\n",
      " [-0.02]]\n",
      "üåÄ Velocidades aplicadas aos vieses, funcionando como um 'ac√∫mulo' de dire√ß√£o para suavizar e acelerar a converg√™ncia.\n",
      "\n",
      "üöÄ Camada 2 - VELOCIDADES DOS VIESES\n",
      "[[-0.03]]\n",
      "üåÄ Velocidades aplicadas aos vieses, funcionando como um 'ac√∫mulo' de dire√ß√£o para suavizar e acelerar a converg√™ncia.\n",
      "\n",
      "================= FIM =================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementa√ß√£o do gradiente descendente com momentum\n",
    "def update_with_momentum(weights, biases, deltas, activations, learning_rate, velocities, momentum=0.9):\n",
    "    \"\"\"Atualiza os pesos e vieses usando o gradiente descendente com momentum\n",
    "    \n",
    "    Args:\n",
    "        weights: Lista de matrizes de pesos para cada camada\n",
    "        biases: Lista de vetores de vi√©s para cada camada\n",
    "        deltas: Lista de erros para cada camada\n",
    "        activations: Lista de ativa√ß√µes para cada camada\n",
    "        learning_rate: Taxa de aprendizado\n",
    "        velocities: Lista de velocidades para cada camada (pesos e vieses)\n",
    "        momentum: Coeficiente de momentum\n",
    "        \n",
    "    Returns:\n",
    "        weights: Pesos atualizados\n",
    "        biases: Vieses atualizados\n",
    "        velocities: Velocidades atualizadas\n",
    "    \"\"\"\n",
    "    # Descompactar velocidades\n",
    "    velocity_w, velocity_b = velocities\n",
    "    \n",
    "    # Atualizar pesos e vieses para cada camada\n",
    "    for l in range(len(weights)):\n",
    "        # Calcular gradientes\n",
    "        grad_w = np.dot(deltas[l], activations[l].transpose())\n",
    "        grad_b = deltas[l]\n",
    "        \n",
    "        # Atualizar velocidades\n",
    "        velocity_w[l] = momentum * velocity_w[l] - learning_rate * grad_w\n",
    "        velocity_b[l] = momentum * velocity_b[l] - learning_rate * grad_b\n",
    "        \n",
    "        # Atualizar pesos e vieses\n",
    "        weights[l] = weights[l] + velocity_w[l]\n",
    "        biases[l] = biases[l] + velocity_b[l]\n",
    "    \n",
    "    return weights, biases, (velocity_w, velocity_b)\n",
    "\n",
    "# Exemplo de uso\n",
    "# Inicializar velocidades com zeros\n",
    "velocity_w = [np.zeros_like(w) for w in weights]\n",
    "velocity_b = [np.zeros_like(b) for b in biases]\n",
    "velocities = (velocity_w, velocity_b)\n",
    "\n",
    "# Atualizar pesos e vieses com momentum\n",
    "new_weights, new_biases, new_velocities = update_with_momentum(weights, biases, deltas, activations, learning_rate, velocities)\n",
    "\n",
    "\n",
    "print(\"\\n================= RESULTADOS DA ATUALIZA√á√ÉO COM MOMENTUM =================\\n\")\n",
    "\n",
    "# Pesos\n",
    "for i in range(len(new_weights)):\n",
    "    print(f\"Camada {i+1} - PESOS\")\n",
    "    print(\"üî∏ Pesos atualizados com momentum:\")\n",
    "    print(new_weights[i])\n",
    "    print(\"Interpreta√ß√£o:\")\n",
    "    print(\n",
    "        f\"Nesta camada, os pesos foram atualizados considerando n√£o apenas o gradiente atual \"\n",
    "        f\"(que aponta na dire√ß√£o do erro atual), mas tamb√©m a velocidade acumulada de atualiza√ß√µes \"\n",
    "        f\"anteriores. O momentum (coeficiente = {0.9}) faz com que a rede tenha 'in√©rcia' nas atualiza√ß√µes, \"\n",
    "        f\"ajudando a acelerar nas dire√ß√µes corretas e a reduzir oscila√ß√µes, especialmente em vales estreitos \"\n",
    "        f\"do espa√ßo de erro.\\n\"\n",
    "    )\n",
    "\n",
    "# Vieses\n",
    "for i in range(len(new_biases)):\n",
    "    print(f\"Camada {i+1} - VIESES\")\n",
    "    print(\"üî∏ Vieses atualizados com momentum:\")\n",
    "    print(new_biases[i])\n",
    "    print(\"Interpreta√ß√£o:\")\n",
    "    print(\n",
    "        f\"Os vieses foram atualizados com a mesma l√≥gica dos pesos: al√©m do gradiente atual, \"\n",
    "        f\"a velocidade anterior influencia a dire√ß√£o e o tamanho do passo. Isso permite um \"\n",
    "        f\"avan√ßo mais suave e r√°pido na superf√≠cie de erro, evitando quedas abruptas e ajudando \"\n",
    "        f\"na estabilidade da aprendizagem.\\n\"\n",
    "    )\n",
    "\n",
    "# Velocidades\n",
    "for i in range(len(new_velocities[0])):\n",
    "    print(f\"Camada {i+1} - VELOCIDADES DOS PESOS\")\n",
    "    print(new_velocities[0][i])\n",
    "    print(\"Estas velocidades representam a combina√ß√£o entre a velocidade anterior \"\n",
    "          \"e o gradiente atual, ponderadas pelo momentum. Quanto maior o momentum, \"\n",
    "          \"mais a velocidade anterior influencia.\\n\")\n",
    "\n",
    "for i in range(len(new_velocities[1])):\n",
    "    print(f\"Camada {i+1} - VELOCIDADES DOS VIESES\")\n",
    "    print(new_velocities[1][i])\n",
    "    print(\"Velocidades aplicadas aos vieses, funcionando como um 'ac√∫mulo' de dire√ß√£o \"\n",
    "          \"para suavizar e acelerar a converg√™ncia.\\n\")\n",
    "\n",
    "print(\"================= FIM =================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo\n",
    "\n",
    "1. O erro em uma camada oculta √© calculado como a soma ponderada dos erros na camada seguinte, multiplicada pela derivada da fun√ß√£o de ativa√ß√£o: $\\delta_j^l = \\left( \\sum_{k} w_{kj}^{l+1} \\delta_k^{l+1} \\right) \\cdot f'(z_j^l)$.\n",
    "\n",
    "2. A atualiza√ß√£o dos pesos √© feita usando o gradiente descendente: $w_{ji}^l = w_{ji}^l - \\eta \\delta_j^l a_i^{l-1}$.\n",
    "\n",
    "3. Variantes como o gradiente descendente com momentum podem melhorar a converg√™ncia e o desempenho do algoritmo.\n",
    "\n",
    "4. A escolha da taxa de aprendizado √© crucial para o sucesso do treinamento da rede neural."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
