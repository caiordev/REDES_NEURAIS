{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fórmulas Importantes no Backpropagation - Parte 2\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Neste notebook, exploraremos as fórmulas matemáticas para o cálculo do erro nas camadas ocultas e a atualização dos pesos durante o algoritmo de Backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display, Math\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erro nas Camadas Ocultas\n",
    "\n",
    "Enquanto o erro na camada de saída é calculado diretamente comparando a saída da rede com o valor desejado, o erro nas camadas ocultas é calculado propagando o erro da camada seguinte para trás.\n",
    "\n",
    "### Derivação da Fórmula\n",
    "\n",
    "Para um neurônio $j$ na camada $l$, o erro $\\delta_j^l$ é definido como a derivada parcial do erro total em relação à soma ponderada $z_j^l$:\n",
    "\n",
    "$\\delta_j^l = \\frac{\\partial E}{\\partial z_j^l}$\n",
    "\n",
    "Usando a regra da cadeia, podemos expressar este erro em termos do erro na camada seguinte $l+1$:\n",
    "\n",
    "$\\delta_j^l = \\left( \\sum_{k} w_{kj}^{l+1} \\delta_k^{l+1} \\right) \\cdot f'(z_j^l)$\n",
    "\n",
    "Onde:\n",
    "- $w_{kj}^{l+1}$ é o peso da conexão entre o neurônio $j$ na camada $l$ e o neurônio $k$ na camada $l+1$\n",
    "- $\\delta_k^{l+1}$ é o erro do neurônio $k$ na camada $l+1$\n",
    "- $f'(z_j^l)$ é a derivada da função de ativação avaliada em $z_j^l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= INICIANDO CÁLCULO DO ERRO NA CAMADA OCULTA =================\n",
      "\n",
      "Dados de entrada:\n",
      " Erro na camada seguinte (delta_next):\n",
      "[[0.1]\n",
      " [0.2]\n",
      " [0.3]]\n",
      " Pesos da camada atual para a próxima (weights_next):\n",
      "[[0.1 0.2]\n",
      " [0.3 0.4]\n",
      " [0.5 0.6]]\n",
      " Soma ponderada das entradas da camada atual (z_current):\n",
      "[[0.5]\n",
      " [0.7]]\n",
      " Função de ativação utilizada: sigmoid\n",
      "\n",
      " Contribuição do erro da camada seguinte (peso.T x delta_next):\n",
      "[[0.22]\n",
      " [0.28]]\n",
      "\n",
      " Derivada da função de ativação aplicada à camada atual:\n",
      "[[0.23500371]\n",
      " [0.22171287]]\n",
      "\n",
      " Resultado final - Erro na camada atual (delta):\n",
      "[[0.05170082]\n",
      " [0.0620796 ]]\n",
      "\n",
      "================= FIM DO CÁLCULO =================\n",
      "\n",
      "Delta (erro) na camada atual calculado com sucesso:\n",
      "[[0.05170082]\n",
      " [0.0620796 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Funções auxiliares\n",
    "def sigmoid(z):\n",
    "    \"\"\"Função de ativação sigmoid\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivada da função sigmoid\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# Cálculo do erro na camada oculta\n",
    "def hidden_layer_error(delta_next, weights_next, z_current, activation_function='sigmoid'):\n",
    "    \"\"\"Calcula o erro para uma camada oculta\"\"\"\n",
    "    \n",
    "    print(\"\\n================= INICIANDO CÁLCULO DO ERRO NA CAMADA OCULTA =================\")\n",
    "    \n",
    "    print(\"\\nDados de entrada:\")\n",
    "    print(f\" Erro na camada seguinte (delta_next):\\n{delta_next}\")\n",
    "    print(f\" Pesos da camada atual para a próxima (weights_next):\\n{weights_next}\")\n",
    "    print(f\" Soma ponderada das entradas da camada atual (z_current):\\n{z_current}\")\n",
    "    print(f\" Função de ativação utilizada: {activation_function}\")\n",
    "    \n",
    "    # 1. Calcula contribuição do erro da próxima camada\n",
    "    error_contribution = np.dot(weights_next.T, delta_next)\n",
    "    print(\"\\n Contribuição do erro da camada seguinte (peso.T x delta_next):\")\n",
    "    print(error_contribution)\n",
    "    \n",
    "    # 2. Calcula a derivada da função de ativação\n",
    "    if activation_function == 'sigmoid':\n",
    "        activation_derivative = sigmoid_derivative(z_current)\n",
    "    elif activation_function == 'tanh':\n",
    "        activation_derivative = 1 - np.tanh(z_current) ** 2\n",
    "    elif activation_function == 'relu':\n",
    "        activation_derivative = np.where(z_current > 0, 1, 0)\n",
    "    else:\n",
    "        raise ValueError(f\"Função de ativação '{activation_function}' não suportada\")\n",
    "    \n",
    "    print(\"\\n Derivada da função de ativação aplicada à camada atual:\")\n",
    "    print(activation_derivative)\n",
    "    \n",
    "    # 3. Calcula o erro da camada atual\n",
    "    delta = error_contribution * activation_derivative\n",
    "    \n",
    "    print(\"\\n Resultado final - Erro na camada atual (delta):\")\n",
    "    print(delta)\n",
    "    \n",
    "    print(\"\\n================= FIM DO CÁLCULO =================\\n\")\n",
    "    return delta\n",
    "\n",
    "# ================= EXEMPLO DE USO =================\n",
    "# Rede com 3 neurônios na camada l+1 e 2 neurônios na camada l\n",
    "delta_next = np.array([[0.1], [0.2], [0.3]])  # Erro na camada seguinte (l+1)\n",
    "weights_next = np.array([[0.1, 0.2],   # Pesos de cada neurônio da camada l+1 para camada l\n",
    "                          [0.3, 0.4],\n",
    "                          [0.5, 0.6]])\n",
    "z_current = np.array([[0.5], [0.7]])  # Soma ponderada na camada atual (l)\n",
    "\n",
    "# Cálculo do erro na camada atual (l)\n",
    "delta_current = hidden_layer_error(delta_next, weights_next, z_current, activation_function='sigmoid')\n",
    "\n",
    "print(\"Delta (erro) na camada atual calculado com sucesso:\")\n",
    "print(delta_current)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atualização dos Pesos\n",
    "\n",
    "Após calcular os erros para cada neurônio na rede, o próximo passo é atualizar os pesos para minimizar o erro. Isso é feito usando o algoritmo do gradiente descendente ou suas variantes.\n",
    "\n",
    "### Derivação da Fórmula\n",
    "\n",
    "Para atualizar um peso $w_{ji}^l$ que conecta o neurônio $i$ na camada $l-1$ ao neurônio $j$ na camada $l$, a fórmula é:\n",
    "\n",
    "$w_{ji}^l = w_{ji}^l - \\eta \\frac{\\partial E}{\\partial w_{ji}^l} = w_{ji}^l - \\eta \\delta_j^l a_i^{l-1}$\n",
    "\n",
    "Onde:\n",
    "- $\\eta$ é a taxa de aprendizado\n",
    "- $\\delta_j^l$ é o erro do neurônio $j$ na camada $l$\n",
    "- $a_i^{l-1}$ é a ativação do neurônio $i$ na camada $l-1$\n",
    "\n",
    "De forma similar, a atualização dos vieses (bias) é dada por:\n",
    "\n",
    "$b_j^l = b_j^l - \\eta \\delta_j^l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= RESULTADOS DA ATUALIZAÇÃO =================\n",
      "\n",
      " Camada 1 - PESOS\n",
      " Pesos originais:\n",
      "[[0.1 0.2]\n",
      " [0.3 0.4]]\n",
      " Pesos atualizados:\n",
      "[[0.095 0.194]\n",
      " [0.29  0.388]]\n",
      " Interpretação: Cada valor foi ajustado na direção de reduzir o erro da rede. A atualização ocorre proporcional ao gradiente (delta) e à ativação da camada anterior.\n",
      "\n",
      " Camada 2 - PESOS\n",
      " Pesos originais:\n",
      "[[0.5 0.6]]\n",
      " Pesos atualizados:\n",
      "[[0.479 0.576]]\n",
      " Interpretação: Cada valor foi ajustado na direção de reduzir o erro da rede. A atualização ocorre proporcional ao gradiente (delta) e à ativação da camada anterior.\n",
      "\n",
      " Camada 1 - VIESES\n",
      " Vieses originais:\n",
      "[[0.1]\n",
      " [0.2]]\n",
      " Vieses atualizados:\n",
      "[[0.09]\n",
      " [0.18]]\n",
      " Interpretação: Os vieses foram ajustados diretamente na proporção do delta da camada, independentemente da ativação, pois o viés não depende da entrada.\n",
      "\n",
      " Camada 2 - VIESES\n",
      " Vieses originais:\n",
      "[[0.3]]\n",
      " Vieses atualizados:\n",
      "[[0.27]]\n",
      " Interpretação: Os vieses foram ajustados diretamente na proporção do delta da camada, independentemente da ativação, pois o viés não depende da entrada.\n",
      "\n",
      "================= FIM =================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Implementação da atualização de pesos e vieses\n",
    "# =========================\n",
    "\n",
    "def update_weights_and_biases(weights, biases, deltas, activations, learning_rate):\n",
    "    \"\"\"Atualiza os pesos e vieses usando o gradiente descendente\"\"\"\n",
    "    for l in range(len(weights)):\n",
    "        weights[l] = weights[l] - learning_rate * np.dot(deltas[l], activations[l].transpose())\n",
    "        biases[l] = biases[l] - learning_rate * deltas[l]\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Exemplo de uso\n",
    "# =========================\n",
    "\n",
    "# Definir uma rede neural simples com 2 camadas\n",
    "weights = [np.array([[0.1, 0.2], [0.3, 0.4]]), np.array([[0.5, 0.6]])]\n",
    "biases = [np.array([[0.1], [0.2]]), np.array([[0.3]])]\n",
    "\n",
    "# Definir ativações e erros (deltas)\n",
    "activations = [np.array([[0.5], [0.6]]), np.array([[0.7], [0.8]]), np.array([[0.9]])]\n",
    "deltas = [np.array([[0.1], [0.2]]), np.array([[0.3]])]\n",
    "\n",
    "# Definir taxa de aprendizado\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Salvar os pesos e vieses originais para comparação\n",
    "original_weights = [w.copy() for w in weights]\n",
    "original_biases = [b.copy() for b in biases]\n",
    "\n",
    "# Atualizar pesos e vieses\n",
    "new_weights, new_biases = update_weights_and_biases(weights, biases, deltas, activations, learning_rate)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Impressão detalhada dos resultados\n",
    "# =========================\n",
    "\n",
    "print(\"\\n================= RESULTADOS DA ATUALIZAÇÃO =================\\n\")\n",
    "\n",
    "# Pesos\n",
    "for i in range(len(original_weights)):\n",
    "    print(f\" Camada {i+1} - PESOS\")\n",
    "    print(\" Pesos originais:\")\n",
    "    print(original_weights[i])\n",
    "    print(\" Pesos atualizados:\")\n",
    "    print(new_weights[i])\n",
    "    print(\" Interpretação: Cada valor foi ajustado na direção de reduzir o erro da rede. \"\n",
    "          \"A atualização ocorre proporcional ao gradiente (delta) e à ativação da camada anterior.\\n\")\n",
    "\n",
    "# Vieses\n",
    "for i in range(len(original_biases)):\n",
    "    print(f\" Camada {i+1} - VIESES\")\n",
    "    print(\" Vieses originais:\")\n",
    "    print(original_biases[i])\n",
    "    print(\" Vieses atualizados:\")\n",
    "    print(new_biases[i])\n",
    "    print(\" Interpretação: Os vieses foram ajustados diretamente na proporção do delta da camada, \"\n",
    "          \"independentemente da ativação, pois o viés não depende da entrada.\\n\")\n",
    "\n",
    "print(\"================= FIM =================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variantes do Gradiente Descendente\n",
    "\n",
    "Existem várias variantes do algoritmo do gradiente descendente que podem melhorar a convergência e o desempenho:\n",
    "\n",
    "### Gradiente Descendente com Momentum\n",
    "\n",
    "Adiciona um termo de momentum que ajuda a acelerar a convergência e evitar mínimos locais:\n",
    "\n",
    "$v = \\gamma v - \\eta \\nabla E$\n",
    "$w = w + v$\n",
    "\n",
    "Onde $v$ é o vetor de velocidade e $\\gamma$ é o coeficiente de momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= RESULTADOS DA ATUALIZAÇÃO COM MOMENTUM =================\n",
      "\n",
      "Camada 1 - PESOS\n",
      " Pesos atualizados com momentum:\n",
      "[[0.09  0.188]\n",
      " [0.28  0.376]]\n",
      "Interpretação:\n",
      "Nesta camada, os pesos foram atualizados considerando não apenas o gradiente atual (que aponta na direção do erro atual), mas também a velocidade acumulada de atualizações anteriores. O momentum (coeficiente = 0.9) faz com que a rede tenha 'inércia' nas atualizações, ajudando a acelerar nas direções corretas e a reduzir oscilações, especialmente em vales estreitos do espaço de erro.\n",
      "\n",
      "Camada 2 - PESOS\n",
      " Pesos atualizados com momentum:\n",
      "[[0.458 0.552]]\n",
      "Interpretação:\n",
      "Nesta camada, os pesos foram atualizados considerando não apenas o gradiente atual (que aponta na direção do erro atual), mas também a velocidade acumulada de atualizações anteriores. O momentum (coeficiente = 0.9) faz com que a rede tenha 'inércia' nas atualizações, ajudando a acelerar nas direções corretas e a reduzir oscilações, especialmente em vales estreitos do espaço de erro.\n",
      "\n",
      "Camada 1 - VIESES\n",
      " Vieses atualizados com momentum:\n",
      "[[0.08]\n",
      " [0.16]]\n",
      "Interpretação:\n",
      "Os vieses foram atualizados com a mesma lógica dos pesos: além do gradiente atual, a velocidade anterior influencia a direção e o tamanho do passo. Isso permite um avanço mais suave e rápido na superfície de erro, evitando quedas abruptas e ajudando na estabilidade da aprendizagem.\n",
      "\n",
      "Camada 2 - VIESES\n",
      " Vieses atualizados com momentum:\n",
      "[[0.24]]\n",
      "Interpretação:\n",
      "Os vieses foram atualizados com a mesma lógica dos pesos: além do gradiente atual, a velocidade anterior influencia a direção e o tamanho do passo. Isso permite um avanço mais suave e rápido na superfície de erro, evitando quedas abruptas e ajudando na estabilidade da aprendizagem.\n",
      "\n",
      "Camada 1 - VELOCIDADES DOS PESOS\n",
      "[[-0.005 -0.006]\n",
      " [-0.01  -0.012]]\n",
      "Estas velocidades representam a combinação entre a velocidade anterior e o gradiente atual, ponderadas pelo momentum. Quanto maior o momentum, mais a velocidade anterior influencia.\n",
      "\n",
      "Camada 2 - VELOCIDADES DOS PESOS\n",
      "[[-0.021 -0.024]]\n",
      "Estas velocidades representam a combinação entre a velocidade anterior e o gradiente atual, ponderadas pelo momentum. Quanto maior o momentum, mais a velocidade anterior influencia.\n",
      "\n",
      "Camada 1 - VELOCIDADES DOS VIESES\n",
      "[[-0.01]\n",
      " [-0.02]]\n",
      "Velocidades aplicadas aos vieses, funcionando como um 'acúmulo' de direção para suavizar e acelerar a convergência.\n",
      "\n",
      "Camada 2 - VELOCIDADES DOS VIESES\n",
      "[[-0.03]]\n",
      "Velocidades aplicadas aos vieses, funcionando como um 'acúmulo' de direção para suavizar e acelerar a convergência.\n",
      "\n",
      "================= FIM =================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementação do gradiente descendente com momentum\n",
    "def update_with_momentum(weights, biases, deltas, activations, learning_rate, velocities, momentum=0.9):\n",
    "    \"\"\"Atualiza os pesos e vieses usando o gradiente descendente com momentum\n",
    "    \n",
    "    Args:\n",
    "        weights: Lista de matrizes de pesos para cada camada\n",
    "        biases: Lista de vetores de viés para cada camada\n",
    "        deltas: Lista de erros para cada camada\n",
    "        activations: Lista de ativações para cada camada\n",
    "        learning_rate: Taxa de aprendizado\n",
    "        velocities: Lista de velocidades para cada camada (pesos e vieses)\n",
    "        momentum: Coeficiente de momentum\n",
    "        \n",
    "    Returns:\n",
    "        weights: Pesos atualizados\n",
    "        biases: Vieses atualizados\n",
    "        velocities: Velocidades atualizadas\n",
    "    \"\"\"\n",
    "    # Descompactar velocidades\n",
    "    velocity_w, velocity_b = velocities\n",
    "    \n",
    "    # Atualizar pesos e vieses para cada camada\n",
    "    for l in range(len(weights)):\n",
    "        # Calcular gradientes\n",
    "        grad_w = np.dot(deltas[l], activations[l].transpose())\n",
    "        grad_b = deltas[l]\n",
    "        \n",
    "        # Atualizar velocidades\n",
    "        velocity_w[l] = momentum * velocity_w[l] - learning_rate * grad_w\n",
    "        velocity_b[l] = momentum * velocity_b[l] - learning_rate * grad_b\n",
    "        \n",
    "        # Atualizar pesos e vieses\n",
    "        weights[l] = weights[l] + velocity_w[l]\n",
    "        biases[l] = biases[l] + velocity_b[l]\n",
    "    \n",
    "    return weights, biases, (velocity_w, velocity_b)\n",
    "\n",
    "# Exemplo de uso\n",
    "# Inicializar velocidades com zeros\n",
    "velocity_w = [np.zeros_like(w) for w in weights]\n",
    "velocity_b = [np.zeros_like(b) for b in biases]\n",
    "velocities = (velocity_w, velocity_b)\n",
    "\n",
    "# Atualizar pesos e vieses com momentum\n",
    "new_weights, new_biases, new_velocities = update_with_momentum(weights, biases, deltas, activations, learning_rate, velocities)\n",
    "\n",
    "\n",
    "print(\"\\n================= RESULTADOS DA ATUALIZAÇÃO COM MOMENTUM =================\\n\")\n",
    "\n",
    "# Pesos\n",
    "for i in range(len(new_weights)):\n",
    "    print(f\"Camada {i+1} - PESOS\")\n",
    "    print(\" Pesos atualizados com momentum:\")\n",
    "    print(new_weights[i])\n",
    "    print(\"Interpretação:\")\n",
    "    print(\n",
    "        f\"Nesta camada, os pesos foram atualizados considerando não apenas o gradiente atual \"\n",
    "        f\"(que aponta na direção do erro atual), mas também a velocidade acumulada de atualizações \"\n",
    "        f\"anteriores. O momentum (coeficiente = {0.9}) faz com que a rede tenha 'inércia' nas atualizações, \"\n",
    "        f\"ajudando a acelerar nas direções corretas e a reduzir oscilações, especialmente em vales estreitos \"\n",
    "        f\"do espaço de erro.\\n\"\n",
    "    )\n",
    "\n",
    "# Vieses\n",
    "for i in range(len(new_biases)):\n",
    "    print(f\"Camada {i+1} - VIESES\")\n",
    "    print(\" Vieses atualizados com momentum:\")\n",
    "    print(new_biases[i])\n",
    "    print(\"Interpretação:\")\n",
    "    print(\n",
    "        f\"Os vieses foram atualizados com a mesma lógica dos pesos: além do gradiente atual, \"\n",
    "        f\"a velocidade anterior influencia a direção e o tamanho do passo. Isso permite um \"\n",
    "        f\"avanço mais suave e rápido na superfície de erro, evitando quedas abruptas e ajudando \"\n",
    "        f\"na estabilidade da aprendizagem.\\n\"\n",
    "    )\n",
    "\n",
    "# Velocidades\n",
    "for i in range(len(new_velocities[0])):\n",
    "    print(f\"Camada {i+1} - VELOCIDADES DOS PESOS\")\n",
    "    print(new_velocities[0][i])\n",
    "    print(\"Estas velocidades representam a combinação entre a velocidade anterior \"\n",
    "          \"e o gradiente atual, ponderadas pelo momentum. Quanto maior o momentum, \"\n",
    "          \"mais a velocidade anterior influencia.\\n\")\n",
    "\n",
    "for i in range(len(new_velocities[1])):\n",
    "    print(f\"Camada {i+1} - VELOCIDADES DOS VIESES\")\n",
    "    print(new_velocities[1][i])\n",
    "    print(\"Velocidades aplicadas aos vieses, funcionando como um 'acúmulo' de direção \"\n",
    "          \"para suavizar e acelerar a convergência.\\n\")\n",
    "\n",
    "print(\"================= FIM =================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo\n",
    "\n",
    "1. O erro em uma camada oculta é calculado como a soma ponderada dos erros na camada seguinte, multiplicada pela derivada da função de ativação: $\\delta_j^l = \\left( \\sum_{k} w_{kj}^{l+1} \\delta_k^{l+1} \\right) \\cdot f'(z_j^l)$.\n",
    "\n",
    "2. A atualização dos pesos é feita usando o gradiente descendente: $w_{ji}^l = w_{ji}^l - \\eta \\delta_j^l a_i^{l-1}$.\n",
    "\n",
    "3. Variantes como o gradiente descendente com momentum podem melhorar a convergência e o desempenho do algoritmo.\n",
    "\n",
    "4. A escolha da taxa de aprendizado é crucial para o sucesso do treinamento da rede neural."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
